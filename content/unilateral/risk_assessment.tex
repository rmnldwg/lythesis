\providecommand{\relativeRoot}{../..}
\documentclass[\relativeRoot/main.tex]{subfiles}


\begin{document}

\section{Risk assessment of microscopic involvement}
\label{sec:unilateral:risk_assessment}

With a parameter set $\hat{\theta} = \left( \big\{ \Tilde{b}_v \big\}, \big\{ \Tilde{t}_{rv} \big\}_{r \in \pa(v)} \right) \ \forall v \leq V$, we can assess the risk of nodal involvement, given a diagnosis $\mathbf{z}$, of a new patient. Using Bayes' law, the risk for a certain \gls{lnl} $v$ being involved is given by the conditional probability
%
\begin{equation} \label{eq:unilateral:sampling:risk}
    \begin{aligned}
        R \left( X_v=1 \mid \mathbf{z}, \hat{\theta} \right) 
        &= \frac{P \left( \mathbf{Z}=\mathbf{z} \mid X_v=1, \hat{\theta} \right) P \left( X_v=1 \mid \hat{\theta} \right)}{P \left( \mathbf{Z}=\mathbf{z} \mid \hat{\theta} \right)} \\
        &= \sum_{i\,:\,\xi_{iv}=1}{\frac{P \left( \mathbf{Z}=\mathbf{z} \mid \boldsymbol{\xi}_i , \hat{\theta} \right) P \left( \boldsymbol{\xi}_i \mid \hat{\theta} \right)}{P \left( \mathbf{Z}=\mathbf{z} \mid \hat{\theta} \right)}}
    \end{aligned}
\end{equation}
%
Note that in the second line, we have explicitly written out the marginalization over all hidden states $\boldsymbol{\xi}_i$ that have \gls{lnl} $v$ involved. We have written the state of \gls{lnl} $v$ in the state $\boldsymbol{\xi}_i$ as $\xi_{iv}$. The denominator can be computed using \cref{eq:unilateral:hmm_marginalize}, which already includes the marginalization over all hidden states $\boldsymbol{\xi}_i$.

Instead of just using one estimate of a parameter set $\hat{\theta}$ however, we are going to employ \gls{mcmc}. This allows us to draw $L$ sets of parameters $\boldsymbol{\hat{\theta}} = \begin{pmatrix} \hat{\theta}_1 & \hat{\theta}_2 & \ldots & \hat{\theta}_L \end{pmatrix}$ that are distributed like the posterior $\Probofgiven{\hat{\theta}}{\boldsymbol{\mathcal{Z}}}$. Since those sampled $\boldsymbol{\hat{\theta}}$ are random variables, the risk $R \left( X_v \mid \mathbf{z}, \hat{\theta} \right)$ being a function of $\hat{\theta}$ is too. Using the Monte Carlo estimator, we can therefore compute the moments of the distribution over the risk, including e.g. the expectation value
%
\begin{equation}
    \mathbb{E}_{\boldsymbol{\hat{\theta}}} \left[ R \left( X_v = 1 \mid \mathbf{z} \right) \right] = \frac{1}{L} \sum_{k=1}^{L}{R \left( X_v = 1 \mid \mathbf{z}, \hat{\theta}_k \right)}
\end{equation}
%
In the result sections below, we compute the individual risks for a large number $L$ of sampled parameters. Thereby, we can compute histograms for the risk that will approach the real probability density of the respective risk for $L \rightarrow \infty$. This provides additional information on the uncertainty in the predicted risk resulting from uncertainty in the model parameters.

\end{document}